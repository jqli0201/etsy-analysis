{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc48fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "#load inthe NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
    "from nltk.corpus import stopwords\n",
    "# This allows to create individual objects from a bog of words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# Lemmatizer helps to reduce words to the base form\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5cae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eda=pd.read_csv('eda_np.txt', sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a73888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ebda21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jasmineli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jasmineli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jasmineli/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('summer-products-with-rating-and-performance_2020-08.csv')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8a0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    new_tokens = word_tokenize(sentence)\n",
    "    new_tokens = [t.lower() for t in new_tokens]\n",
    "    new_tokens =[t for t in new_tokens if t not in stopwords.words('english')]\n",
    "    new_tokens = [t for t in new_tokens if t.isalpha()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_tokens =[lemmatizer.lemmatize(t) for t in new_tokens]\n",
    "    # return \" \".join(new_tokens), len(new_tokens)\n",
    "    return \" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0d31ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tags_pre'] = df['tags'].apply(lambda x: x.replace(',', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df['title_orig'].tolist()\n",
    "# results = [process_sentence(t) for t in titles]\n",
    "tokens = [process_sentence(t) for t in titles]\n",
    "# tokens, length = zip(*results)\n",
    "df['title_pre'] = tokens\n",
    "# df['title_len'] = df['title_pre'].str.split(\" \").str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000686a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['title_len', 'units_sold']].sort_values(by='units_sold', ascending=False).head(10).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['title_len', 'units_sold']].sort_values(by='units_sold', ascending=False).tail(10).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tags = df['tags_pre'].tolist()\n",
    "# results = [process_sentence(t) for t in tags]\n",
    "# tokens, length = zip(*results)\n",
    "# df['tags_pre'] = tokens\n",
    "# df['tags_num'] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['tags_num', 'units_sold']].sort_values(by='units_sold', ascending=False).head(30).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e15cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['tags_num', 'units_sold']].sort_values(by='units_sold', ascending=False).tail(30).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a29e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(df['title_pre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "tfidf = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b55dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "# vectors = vectorizer.fit_transform(df['tags_pre'])\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "# dense = vectors.todense()\n",
    "# denselist = dense.tolist()\n",
    "# tfidf_tag = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8156d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product color\n",
    "def main_color(s):\n",
    "    main_color = {\"red\":\"red\", \"white\":\"white\", \"pink\":\"pink\", \"yellow\":\"yellow\", \"green\":\"green\", \"blue\":\"blue\", \"wine\":\"red\", \"burgundy\":\"red\", \"black\":\"black\", \"navy\":\"navy\", \"orange\":\"orange\", \n",
    "    \"rose\":\"pink\", \"gray\":\"gray\", \"grey\":\"gray\", \"purple\":\"purple\", \"violet\":\"purple\", \"army\":\"green\", \"leopard\":\"orange\", \"ivory\":\"white\", \n",
    "    \"brown\":\"brown\", \"coffee\":\"brown\", \"camel\":\"beige\", \"tan\":\"brown\", \"nude\":\"beige\", \"khaki\":\"khaki\", \"apricot\":\"yellow\", \"camouflage\":\"green\", \"jasper\":\"red\"}  # ordered by importance\n",
    "    for key, value in main_color.items():\n",
    "        if key in s:\n",
    "            return value\n",
    "    return \"others\"\n",
    "product_color = df[\"product_color\"]\n",
    "product_color = [s.lower() if type(s) is str else 'nan' for s in product_color]\n",
    "product_color = [main_color(s) for s in product_color]\n",
    "from matplotlib import colors\n",
    "product_color = [(-0.1,-0.1,-0.1,-0.1) if s == \"others\" else colors.to_rgba(s) for s in product_color]\n",
    "\n",
    "df['product_color_rgb'] = [np.array(t) for t in product_color]\n",
    "\n",
    "# log prices\n",
    "df['log_price'] = [np.log(p) for p in df[\"price\"]]\n",
    "df['log_retail_price'] = [np.log(p) for p in df[\"retail_price\"]]\n",
    "\n",
    "# log merchant rating count\n",
    "df['log_merchant_rating_count'] = np.log(df['merchant_rating_count'])\n",
    "\n",
    "# urgent text\n",
    "df['urgent'] = [1 if s == \"Quantité limitée !\" else 0 for s in df[\"urgency_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d69257",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[[\"log_price\", \"log_retail_price\", \"uses_ad_boosts\", \"badges_count\", \"badge_local_product\", \n",
    "           \"badge_product_quality\", \"badge_fast_shipping\", \"urgent\", \"units_sold\"]]\n",
    "df2 = pd.concat([data, tfidf], axis=1)\n",
    "label = [1 if sales > 100 else 0 for sales in data[\"units_sold\"]]\n",
    "df2['high_sale'] = label\n",
    "print(df2['high_sale'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f48cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = df[\"product_color_rgb\"]\n",
    "rgb = np.stack(rgb.values, axis=0)\n",
    "for i in range(4):\n",
    "    df2[\"product_color_rgb\"+str(i)] = rgb[:,i]\n",
    "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.loc[:, ~df2.columns.isin(['high_sale', 'units_sold'])]\n",
    "# X = tfidf\n",
    "y = df2['high_sale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.125, random_state=42)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.14286, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa30b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of dev examples = \" + str(X_dev.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(y_train.shape))\n",
    "print (\"X_dev shape: \" + str(X_dev.shape))\n",
    "print (\"Y_dev shape: \" + str(y_dev.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f080968",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.expand_dims(X_train, axis=-1)\n",
    "X_dev = tf.expand_dims(X_dev, axis=-1)\n",
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# classifier = LogisticRegression()\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# score = classifier.score(evals_X, evals_y)\n",
    "\n",
    "# print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68feb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D, PReLU, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d313196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=24, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be38e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    verbose=2,\n",
    "    validation_data=(X_dev, y_dev),\n",
    "    batch_size=16,\n",
    "    callbacks=[es]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5f279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (etsy)",
   "language": "python",
   "name": "etsy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
